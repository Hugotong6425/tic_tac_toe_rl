{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# work for random player and human first\n",
    "\n",
    "class Player():\n",
    "    \"\"\"\n",
    "    Abstract player class\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.player_id = None\n",
    "        self.observation = None\n",
    "\n",
    "    def reset(self, player_id):\n",
    "        '''reset to the initial state\n",
    "        '''\n",
    "        self.player_id = player_id\n",
    "        self.observation = None\n",
    "\n",
    "    @staticmethod\n",
    "    def tune_observation_view(observation, player_id):\n",
    "        '''\n",
    "        player_id either 1 or -1. Swap the observation such that 1 means self and -1 means the opponent\n",
    "        e.g.\n",
    "        if player_id = 1, no need to swap the view,\n",
    "        input = array([ 1, -1, 0,\n",
    "                        0,  0, 0,\n",
    "                       -1, -1, 1 ])\n",
    "\n",
    "        output = array([ 1, -1, 0,\n",
    "                         0,  0, 0,\n",
    "                        -1, -1, 1 ])\n",
    "\n",
    "        if player_id = -1, need to swap the view,\n",
    "        input = array([ 1, -1, 0,\n",
    "                        0,  0, 0,\n",
    "                       -1, -1, 1 ])\n",
    "\n",
    "        output = array([-1, 1, 0,\n",
    "                         0, 0, 0,\n",
    "                         1, 1, -1 ])\n",
    "\n",
    "        '''\n",
    "        return observation * player_id\n",
    "    \n",
    "    def observe(self, observation):\n",
    "        '''\n",
    "        receive raw observation from env and tune it if needed\n",
    "        '''\n",
    "        self.observation = self.tune_observation_view(observation, self.player_id)\n",
    "\n",
    "    def pick_action(self, **kwargs):\n",
    "        '''different players have different way to pick an action\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def memorize(self, add_this):\n",
    "        '''some players will jot notes, some will not\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def learn(self, board, **kwargs):\n",
    "        '''some players will study, some will not\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "\n",
    "class Human(Player):\n",
    "    '''\n",
    "    choose this player if you want to play the game\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Human, self).__init__()\n",
    "\n",
    "    def pick_action(self, **kwargs):\n",
    "        cell = input('Pick a cell (top left is 0 and bottom right is 8): \\n')\n",
    "        return int(cell)\n",
    "\n",
    "\n",
    "class Random_player(Player):\n",
    "    \"\"\"\n",
    "    this player will pick random acion for all situation\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Random_player, self).__init__()\n",
    "\n",
    "    def pick_action(self, **kwargs):\n",
    "        possible_action_list = np.argwhere(kwargs['is_action_available'] == 1).reshape([-1])\n",
    "        return np.random.choice(possible_action_list, 1)[0]\n",
    "\n",
    "\n",
    "class Q_player(Player):\n",
    "    def __init__(self, hidden_layers_size, saved_nn_path=None, optimizer='adam', loss='mse'):\n",
    "        self.hidden_layers_size = hidden_layers_size\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.initialize_neural_network(saved_nn_path)\n",
    "       \n",
    "        super(Q_player, self).__init__()    \n",
    "        \n",
    "    def initialize_neural_network(self, saved_nn_path=None):\n",
    "        '''\n",
    "        if saved_nn_path is not None, load the model.\n",
    "        if saved_nn_path is None, initialize the model\n",
    "        '''\n",
    "        if saved_nn_path is None:\n",
    "            x = Input(shape=(9,))\n",
    "            \n",
    "            hidden_result = Dense(self.hidden_layers_size[0], activation='relu', \n",
    "                                  kernel_initializer='he_normal')(x)\n",
    "            if len(self.hidden_layers_size) > 1:\n",
    "                for num_hidden_neurons in self.hidden_layers_size[1:]:\n",
    "                    hidden_result = Dense(num_hidden_neurons, activation='relu', \n",
    "                                          kernel_initializer='he_normal')(hidden_result)\n",
    "                    \n",
    "            y = Dense(9, activation=None)(hidden_result)\n",
    "            \n",
    "            self.brain = Model(inputs=x, outputs=y)\n",
    "            \n",
    "            self.brain.compile(self.optimizer, loss=self.loss)\n",
    "            \n",
    "        else:\n",
    "            #self.brain = keras.load_model()\n",
    "            pass\n",
    "\n",
    "    def pick_action(self, **kwargs):\n",
    "        '''\n",
    "        given 1 observation and is_action_available, predict the best move\n",
    "        \n",
    "        Note: can only predict 1 observation at a time now\n",
    "        '''\n",
    "        observation = kwargs['observation']\n",
    "        print('observation: ', observation)\n",
    "        self.observe(observation)\n",
    "        \n",
    "        is_action_available = kwargs['is_action_available']\n",
    "        \n",
    "        q_pred = player.brain.predict(x=self.observation)\n",
    "        \n",
    "        mask = (is_action_available == 1)\n",
    "\n",
    "        subset_idx = np.argmax(q_pred[mask])\n",
    "\n",
    "        picked_cell = np.arange(q_pred.shape[0])[mask][subset_idx]\n",
    "    \n",
    "        print (picked_cell)\n",
    "        return int(picked_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "player = Q_player([20, 10])\n",
    "player.player_id = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:  [ 1  0  0 -1  0  0  0  0  0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_6 to have shape (9,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-babb67816b3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mis_action_available\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpick_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_action_available\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_action_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-a8dcbb82c35f>\u001b[0m in \u001b[0;36mpick_action\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mis_action_available\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_action_available'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mq_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_action_available\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/general_rl/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/general_rl/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/general_rl/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_6 to have shape (9,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "board = np.array([1,0,0,-1,0,0,0,0,0])#.reshape([1,9])\n",
    "is_action_available = np.array([0,1,1,0,1,1,1,1,1])\n",
    "\n",
    "player.pick_action(is_action_available=is_action_available, observation=board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  0  0 -1  0  0  0  0  0]]\n",
      "(1, 9)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                200       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 9)                 99        \n",
      "=================================================================\n",
      "Total params: 509\n",
      "Trainable params: 509\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[[ 0.16717497 -0.8731156  -0.40747613  0.7763495   0.7876181   0.6127339\n",
      "   0.04668282  0.0815212   0.05382271]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board = np.array([1,0,0,-1,0,0,0,0,0]).reshape([1,9])\n",
    "print(board)\n",
    "print(board.shape)\n",
    "\n",
    "player.brain.summary()\n",
    "\n",
    "print(player.brain.predict(x=board))\n",
    "np.argmax(player.brain.predict(x=board))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from abc import abstractmethod\n",
    "\n",
    "\n",
    "class Player():\n",
    "    \"\"\"\n",
    "    Abstract player class\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.player_id = None\n",
    "        self.observation = None\n",
    "        \n",
    "    def reset(self, player_id):\n",
    "        '''reset to the initial state\n",
    "        '''\n",
    "        self.player_id = player_id\n",
    "        self.observation = None\n",
    "        \n",
    "    def observe(self, observation):\n",
    "        '''\n",
    "        receive raw observation from env and tune it if needed\n",
    "        '''\n",
    "        self.observation = tune_observation_view(observation, self.player_id)\n",
    "        \n",
    "    @staticmethod\n",
    "    def tune_observation_view(observation, player_id):\n",
    "        '''\n",
    "        player_id either 1 or -1. Swap the observation such that 1 means self and -1 means the opponent \n",
    "        e.g. \n",
    "        if player_id = 1, no need to swap the view,\n",
    "        input = array([ 1, -1, 0,\n",
    "                        0,  0, 0,\n",
    "                       -1, -1, 1 ])\n",
    "\n",
    "        output = array([ 1, -1, 0,\n",
    "                         0,  0, 0,\n",
    "                        -1, -1, 1 ])\n",
    "\n",
    "        if player_id = -1, need to swap the view,\n",
    "        input = array([ 1, -1, 0,\n",
    "                        0,  0, 0,\n",
    "                       -1, -1, 1 ])\n",
    "\n",
    "        output = array([-1, 1, 0,\n",
    "                         0, 0, 0,\n",
    "                         1, 1, -1 ])\n",
    "\n",
    "        '''\n",
    "        return observation * player_id\n",
    "\n",
    "    def pick_action(self, observation):\n",
    "        '''different players have different way to pick an action\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def memorize(self, add_this):\n",
    "        '''some players will jot notes, some will not\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def learn(self, board, **kwargs):\n",
    "        '''some players will study, some will not\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    \n",
    "class Human(Player):\n",
    "    '''\n",
    "    choose this player if you want to play the game\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Human, self).__init__()\n",
    "        \n",
    "    def pick_action(self, board, **kwargs):\n",
    "        cell = input('Pick a cell (top left is 0 and bottom right is 8): ')\n",
    "        return cell\n",
    "\n",
    "    \n",
    "class Random_player(Player):\n",
    "    \"\"\"\n",
    "    this player will pick random acion for all situation\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Random_player, self).__init__()\n",
    "        \n",
    "    def pick_action(self, observation, is_action_available):\n",
    "        possible_action_list = np.argwhere(is_action_available == 1).reshape([-1])\n",
    "        return np.random.choice(possible_action_list, 1)\n",
    "    \n",
    "\n",
    "class QPlayer(Player):\n",
    "    \"\"\"\n",
    "    A reinforcement learning agent, based on Double Deep Q Network model\n",
    "    This class holds two Q-Networks: `qnn` is the learning network, `q_target` is the semi-constant network\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_layers_size, gamma, learning_batch_size,\n",
    "                 batches_to_q_target_switch, tau, memory_size):\n",
    "        \"\"\"\n",
    "        :param hidden_layers_size: an array of integers, specifying the number of layers of the network and their size\n",
    "        :param gamma: the Q-Learning discount factor\n",
    "        :param learning_batch_size: training batch size\n",
    "        :param batches_to_q_target_switch: after how many batches (trainings) should the Q-network be copied to Q-Target\n",
    "        :param tau: a number between 0 and 1, determining how to combine the network and Q-Target when copying is performed\n",
    "        :param memory_size: size of the memory buffer used to keep the training set\n",
    "        \"\"\"\n",
    "        self.learning_batch_size = learning_batch_size\n",
    "        self.batches_to_q_target_switch = batches_to_q_target_switch\n",
    "        self.tau = tau\n",
    "        self.learn_counter = 0\n",
    "        self.counter = 0\n",
    "\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)  \n",
    "        self.session = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "        self.memory = dqn.ReplayMemory(memory_size)\n",
    "        self.qnn = dqn.QNetwork(9, 9, hidden_layers_size, gamma)\n",
    "        self.q_target = dqn.QNetwork(9, 9, hidden_layers_size, gamma)\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        super(QPlayer, self).__init__()\n",
    "\n",
    "    def select_cell(self, board, **kwargs):\n",
    "        rnd = random.random()\n",
    "        eps = kwargs['epsilon']\n",
    "        self.counter += 1\n",
    "        if rnd < eps:\n",
    "            cell = random.randint(0,8)\n",
    "            logging.debug(\"Choosing a random cell: %s [Epsilon = %s]\", cell, eps)\n",
    "        else:\n",
    "            prediction = self.session.run(self.qnn.output,feed_dict={self.qnn.states: np.expand_dims(self.player_id * board, axis=0)})\n",
    "            prediction = np.squeeze(prediction)\n",
    "            cell = np.argmax(prediction)\n",
    "            logging.debug(\"Predicting next cell - board: %s | player ID: %s | prediction: %s | cell: %s [Epsilon = %s]\", board, prediction, cell, eps)\n",
    "        return cell\n",
    "\n",
    "    @staticmethod\n",
    "    def _fetch_from_batch(batch, key, enum=False):\n",
    "        if enum:\n",
    "            return np.array(list(enumerate(map(lambda x: x[key], batch))))\n",
    "        else:\n",
    "            return np.array(list(map(lambda x: x[key], batch)))\n",
    "\n",
    "    def learn(self, **kwargs):\n",
    "        logging.debug('Memory counter = %s', self.memory.counter)\n",
    "        self.learn_counter += 1\n",
    "        if self.learn_counter % self.learning_batch_size != 0 or self.memory.counter < self.learning_batch_size:\n",
    "            pass\n",
    "        else:\n",
    "            logging.debug('Starting learning procedure')\n",
    "            batch = self.memory.sample(self.learning_batch_size)\n",
    "            qt = self.session.run(self.q_target.output,feed_dict={self.q_target.states: self._fetch_from_batch(batch,'next_state')})\n",
    "            terminals = self._fetch_from_batch(batch,'game_over')\n",
    "            for i in range(terminals.size):\n",
    "                if terminals[i]:\n",
    "                    qt[i] = np.zeros(9)  # manually setting q-target values of terminal states to 0\n",
    "            lr = kwargs['learning_rate']\n",
    "            _, cost = self.session.run([self.qnn.optimizer, self.qnn.cost],\n",
    "                                       feed_dict={self.qnn.states: self._fetch_from_batch(batch,'state'),\n",
    "                                                  self.qnn.r: self._fetch_from_batch(batch,'reward'),\n",
    "                                                  self.qnn.actions: self._fetch_from_batch(batch, 'action', enum=True),\n",
    "                                                  self.qnn.q_target: qt,\n",
    "                                                  self.qnn.learning_rate: lr})\n",
    "            logging.info('Batch number: %s | Q-Network cost: %s | Learning rate: %s',\n",
    "                         self.learn_counter % self.learning_batch_size, cost, lr)\n",
    "            if self.memory.counter % (self.batches_to_q_target_switch * self.learning_batch_size) == 0:\n",
    "                logging.info('Copying Q-Network to Q-Target')\n",
    "                tf_vars = tf.trainable_variables()\n",
    "                num_of_vars = len(tf_vars)\n",
    "                operations = []\n",
    "                for i,v in enumerate(tf_vars[0:num_of_vars//2]):\n",
    "                    operations.append(tf_vars[i+num_of_vars//2].assign((v.value()*self.tau) + ((1-self.tau)*tf_vars[i+num_of_vars//2].value())))\n",
    "                self.session.run(operations)\n",
    "            return cost\n",
    "\n",
    "    def add_to_memory(self, add_this):\n",
    "        add_this['state'] = self.player_id * add_this['state']\n",
    "        add_this['next_state'] = self.player_id * add_this['next_state']\n",
    "        self.memory.append(add_this)\n",
    "\n",
    "    def save(self, filename):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(self.session, filename)\n",
    "\n",
    "    def restore(self, filename):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(self.session, filename)\n",
    "\n",
    "    def shutdown(self):\n",
    "        self.session.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work for random player and human first\n",
    "\n",
    "class Player():\n",
    "    \"\"\"\n",
    "    Abstract player class\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.player_id = None\n",
    "        self.observation = None\n",
    "        \n",
    "    def reset(self, player_id):\n",
    "        '''reset to the initial state\n",
    "        '''\n",
    "        self.player_id = player_id\n",
    "        self.observation = None\n",
    "        \n",
    "    def observe(self, observation):\n",
    "        '''\n",
    "        receive raw observation from env and tune it if needed\n",
    "        '''\n",
    "        self.observation = tune_observation_view(observation, self.player_id)\n",
    "        \n",
    "    @staticmethod\n",
    "    def tune_observation_view(observation, player_id):\n",
    "        '''\n",
    "        player_id either 1 or -1. Swap the observation such that 1 means self and -1 means the opponent \n",
    "        e.g. \n",
    "        if player_id = 1, no need to swap the view,\n",
    "        input = array([ 1, -1, 0,\n",
    "                        0,  0, 0,\n",
    "                       -1, -1, 1 ])\n",
    "\n",
    "        output = array([ 1, -1, 0,\n",
    "                         0,  0, 0,\n",
    "                        -1, -1, 1 ])\n",
    "\n",
    "        if player_id = -1, need to swap the view,\n",
    "        input = array([ 1, -1, 0,\n",
    "                        0,  0, 0,\n",
    "                       -1, -1, 1 ])\n",
    "\n",
    "        output = array([-1, 1, 0,\n",
    "                         0, 0, 0,\n",
    "                         1, 1, -1 ])\n",
    "\n",
    "        '''\n",
    "        return observation * player_id\n",
    "\n",
    "    def pick_action(self, observation):\n",
    "        '''different players have different way to pick an action\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def memorize(self, add_this):\n",
    "        '''some players will jot notes, some will not\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def learn(self, board, **kwargs):\n",
    "        '''some players will study, some will not\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    \n",
    "class Human(Player):\n",
    "    '''\n",
    "    choose this player if you want to play the game\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Human, self).__init__()\n",
    "        \n",
    "    def pick_action(self, board, **kwargs):\n",
    "        cell = input('Pick a cell (top left is 0 and bottom right is 8): ')\n",
    "        return cell\n",
    "\n",
    "    \n",
    "class Random_player(Player):\n",
    "    \"\"\"\n",
    "    this player will pick random acion for all situation\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Random_player, self).__init__()\n",
    "        \n",
    "    def pick_action(self, is_action_available):\n",
    "        possible_action_list = np.argwhere(is_action_available == 1).reshape([-1])\n",
    "        return np.random.choice(possible_action_list, 1)[0]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_p1 = Random_player()\n",
    "random_p2 = Random_player()\n",
    "\n",
    "random_p1.pick_action(np.array([1,1,1,1,1,1,1,1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-ab283bd9a1f2>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-ab283bd9a1f2>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    for each record:\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def process_memory(memory, is_terminal_state):\n",
    "    if (len(memory) == memory_size + 1) and (is_terminal_state):\n",
    "        # special handle\n",
    "        pass\n",
    "    \n",
    "    # do all the shifting \n",
    "    for each record:\n",
    "        if player is 2:\n",
    "            swap observation, next_observation, reward\n",
    "        \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-83b213627a81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnext_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer_turn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_terminal_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moriginal_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "memory = np.array([])\n",
    "\n",
    "next_observation, player_turn, reward, is_terminal_state = env.step(action)\n",
    "memory.append([original_observation, action, reward, next_observation, player])\n",
    "\n",
    "if (len(memory) == memory_size + 1) or (is_terminal_state):\n",
    "    # shift reward, state, done, and assign 1 for my round and 2 for the opponent round\n",
    "    memory_for_training = process_memory(memory, is_terminal_state)\n",
    "    if is_terminal_state:\n",
    "        memory = np.array([])\n",
    "    else:\n",
    "        memory = memory[-1]\n",
    "        \n",
    "    agent.learn(memory_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main flow\n",
    "\n",
    "load_weight_path = 'dummy_path'\n",
    "model_weight_path = 'dummy_path'\n",
    "\n",
    "env = make_env()\n",
    "bot1 = Bot(load_weight_path)\n",
    "bot2 = Bot()\n",
    "bot_list = [bot1, bot2]\n",
    "unprocess_memory = np.array([])\n",
    "\n",
    "num_episode = 10\n",
    "\n",
    "\n",
    "for episode in range(num_episode):\n",
    "    env.reset()\n",
    "    \n",
    "    # get initial state\n",
    "    observation, player_turn, reward, is_terminal_state = env.get_current_info()\n",
    "    \n",
    "    while not is_terminal_state:\n",
    "        # if now is player 2's turn, swap the observation\n",
    "        if player_turn == 2:\n",
    "            swapped_observation = swap_observation_view(observation)\n",
    "            action = bot_list[player_turn-1].select_action(swapped_observation)\n",
    "        elif player_turn == 1:\n",
    "            action = bot_list[player_turn-1].select_action(observation)\n",
    "        else:\n",
    "            print('Error in player turn. Current player turn is %s.\\n' % player_turn)\n",
    "        \n",
    "        next_observation, next_player_turn, reward, is_terminal_state = env.step(action)\n",
    "        \n",
    "        unprocess_memory.append([observation, action, reward, next_observation, player_turn])\n",
    "        \n",
    "        if (len(unprocess_memory) == memory_size + 1) or (is_terminal_state):\n",
    "            # shift reward, state, done, and assign 1 for my round and 2 for the opponent round\n",
    "            memory_for_training = process_memory(unprocess_memory, is_terminal_state)\n",
    "            if is_terminal_state:\n",
    "                unprocess_memory = np.array([])\n",
    "            else:\n",
    "                unprocess_memory = unprocess_memory[-1]\n",
    "\n",
    "            bot1.learn(memory_for_training)\n",
    "            \n",
    "        observation, player_turn = next_observation, next_player_turn\n",
    "        \n",
    "    # save the model weight periodically\n",
    "    if (episode + 1) % 50 == 0:\n",
    "        bot1.save_weight(model_weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bot():\n",
    "    def __init__(self, load_weight_path=None):\n",
    "        self.observation['board'] = np.zeros([9, 3])\n",
    "        self.observation['1p_inventory'] = np.array([2, 2, 2])\n",
    "        self.observation['2p_inventory'] = np.array([2, 2, 2])\n",
    "                \n",
    "        self.model = self.build_model()\n",
    "        if load_weight_path is not None:\n",
    "            self.model = self.load_weight()\n",
    "        \n",
    "        self\n",
    "        pass\n",
    "    \n",
    "    def build_model(self):\n",
    "        pass\n",
    "    \n",
    "    def load_model(self):\n",
    "        pass\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot1 = Bot()\n",
    "bot2 = Bot()\n",
    "\n",
    "player_list = [bot1, bot2]\n",
    "save_list = []\n",
    "\n",
    "for episode in range(100):\n",
    "    # reset environment\n",
    "    env.reset()\n",
    "    \n",
    "    # start from the 1st player\n",
    "    player_turn = 1\n",
    "    observation = env.observation\n",
    "    while game not end:\n",
    "        action = player_list[state['player_turn']].pick_action(observation)\n",
    "        \n",
    "        # if the action is not valid, the program will be break      \n",
    "        state, next_observation, reward = env.step(action)\n",
    "        \n",
    "        save_list.append([observation, action, next_observation, reward])\n",
    "        observation = next_observation\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'a' must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c4aaf519bf05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'a' must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "np.random.choice(b, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
