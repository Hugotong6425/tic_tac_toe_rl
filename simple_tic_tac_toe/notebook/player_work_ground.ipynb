{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from abc import abstractmethod\n",
    "\n",
    "\n",
    "class Player():\n",
    "    \"\"\"\n",
    "    Abstract player class\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.player_id = None\n",
    "        self.observation = None\n",
    "        \n",
    "    def reset(self, player_id):\n",
    "        '''reset to the initial state\n",
    "        '''\n",
    "        self.player_id = player_id\n",
    "        self.observation = None\n",
    "        \n",
    "    def observe(self, observation):\n",
    "        '''\n",
    "        receive raw observation from env and tune it if needed\n",
    "        '''\n",
    "        self.observation = tune_observation_view(observation, self.player_id)\n",
    "        \n",
    "    @staticmethod\n",
    "    def tune_observation_view(observation, player_id):\n",
    "        '''\n",
    "        player_id either 1 or -1. Swap the observation such that 1 means self and -1 means the opponent \n",
    "        e.g. \n",
    "        if player_id = 1, no need to swap the view,\n",
    "        input = array([ 1, -1, 0,\n",
    "                        0,  0, 0,\n",
    "                       -1, -1, 1 ])\n",
    "\n",
    "        output = array([ 1, -1, 0,\n",
    "                         0,  0, 0,\n",
    "                        -1, -1, 1 ])\n",
    "\n",
    "        if player_id = -1, need to swap the view,\n",
    "        input = array([ 1, -1, 0,\n",
    "                        0,  0, 0,\n",
    "                       -1, -1, 1 ])\n",
    "\n",
    "        output = array([-1, 1, 0,\n",
    "                         0, 0, 0,\n",
    "                         1, 1, -1 ])\n",
    "\n",
    "        '''\n",
    "        return observation * player_id\n",
    "\n",
    "    def pick_action(self, observation):\n",
    "        '''different players have different way to pick an action\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def memorize(self, add_this):\n",
    "        '''some players will jot notes, some will not\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def learn(self, board, **kwargs):\n",
    "        '''some players will study, some will not\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    \n",
    "class Human(Player):\n",
    "    '''\n",
    "    choose this player if you want to play the game\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Human, self).__init__()\n",
    "        \n",
    "    def pick_action(self, board, **kwargs):\n",
    "        cell = input('Pick a cell (top left is 0 and bottom right is 8): ')\n",
    "        return cell\n",
    "\n",
    "    \n",
    "class Random_player(Player):\n",
    "    \"\"\"\n",
    "    this player will pick random acion for all situation\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Random_player, self).__init__()\n",
    "        \n",
    "    def pick_action(self, is_action_available):\n",
    "        possible_action_list = np.argwhere(is_action_available == 1).reshape([-1])\n",
    "        return np.random.choice(possible_action_list, 1)[0]\n",
    "    \n",
    "\n",
    "class QPlayer(Player):\n",
    "    \"\"\"\n",
    "    A reinforcement learning agent, based on Double Deep Q Network model\n",
    "    This class holds two Q-Networks: `qnn` is the learning network, `q_target` is the semi-constant network\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_layers_size, gamma, learning_batch_size,\n",
    "                 batches_to_q_target_switch, tau, memory_size):\n",
    "        \"\"\"\n",
    "        :param hidden_layers_size: an array of integers, specifying the number of layers of the network and their size\n",
    "        :param gamma: the Q-Learning discount factor\n",
    "        :param learning_batch_size: training batch size\n",
    "        :param batches_to_q_target_switch: after how many batches (trainings) should the Q-network be copied to Q-Target\n",
    "        :param tau: a number between 0 and 1, determining how to combine the network and Q-Target when copying is performed\n",
    "        :param memory_size: size of the memory buffer used to keep the training set\n",
    "        \"\"\"\n",
    "        self.learning_batch_size = learning_batch_size\n",
    "        self.batches_to_q_target_switch = batches_to_q_target_switch\n",
    "        self.tau = tau\n",
    "        self.learn_counter = 0\n",
    "        self.counter = 0\n",
    "\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)  \n",
    "        self.session = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "        self.memory = dqn.ReplayMemory(memory_size)\n",
    "        self.qnn = dqn.QNetwork(9, 9, hidden_layers_size, gamma)\n",
    "        self.q_target = dqn.QNetwork(9, 9, hidden_layers_size, gamma)\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        super(QPlayer, self).__init__()\n",
    "\n",
    "    def select_cell(self, board, **kwargs):\n",
    "        rnd = random.random()\n",
    "        eps = kwargs['epsilon']\n",
    "        self.counter += 1\n",
    "        if rnd < eps:\n",
    "            cell = random.randint(0,8)\n",
    "            logging.debug(\"Choosing a random cell: %s [Epsilon = %s]\", cell, eps)\n",
    "        else:\n",
    "            prediction = self.session.run(self.qnn.output,feed_dict={self.qnn.states: np.expand_dims(self.player_id * board, axis=0)})\n",
    "            prediction = np.squeeze(prediction)\n",
    "            cell = np.argmax(prediction)\n",
    "            logging.debug(\"Predicting next cell - board: %s | player ID: %s | prediction: %s | cell: %s [Epsilon = %s]\", board, prediction, cell, eps)\n",
    "        return cell\n",
    "\n",
    "    @staticmethod\n",
    "    def _fetch_from_batch(batch, key, enum=False):\n",
    "        if enum:\n",
    "            return np.array(list(enumerate(map(lambda x: x[key], batch))))\n",
    "        else:\n",
    "            return np.array(list(map(lambda x: x[key], batch)))\n",
    "\n",
    "    def learn(self, **kwargs):\n",
    "        logging.debug('Memory counter = %s', self.memory.counter)\n",
    "        self.learn_counter += 1\n",
    "        if self.learn_counter % self.learning_batch_size != 0 or self.memory.counter < self.learning_batch_size:\n",
    "            pass\n",
    "        else:\n",
    "            logging.debug('Starting learning procedure')\n",
    "            batch = self.memory.sample(self.learning_batch_size)\n",
    "            qt = self.session.run(self.q_target.output,feed_dict={self.q_target.states: self._fetch_from_batch(batch,'next_state')})\n",
    "            terminals = self._fetch_from_batch(batch,'game_over')\n",
    "            for i in range(terminals.size):\n",
    "                if terminals[i]:\n",
    "                    qt[i] = np.zeros(9)  # manually setting q-target values of terminal states to 0\n",
    "            lr = kwargs['learning_rate']\n",
    "            _, cost = self.session.run([self.qnn.optimizer, self.qnn.cost],\n",
    "                                       feed_dict={self.qnn.states: self._fetch_from_batch(batch,'state'),\n",
    "                                                  self.qnn.r: self._fetch_from_batch(batch,'reward'),\n",
    "                                                  self.qnn.actions: self._fetch_from_batch(batch, 'action', enum=True),\n",
    "                                                  self.qnn.q_target: qt,\n",
    "                                                  self.qnn.learning_rate: lr})\n",
    "            logging.info('Batch number: %s | Q-Network cost: %s | Learning rate: %s',\n",
    "                         self.learn_counter % self.learning_batch_size, cost, lr)\n",
    "            if self.memory.counter % (self.batches_to_q_target_switch * self.learning_batch_size) == 0:\n",
    "                logging.info('Copying Q-Network to Q-Target')\n",
    "                tf_vars = tf.trainable_variables()\n",
    "                num_of_vars = len(tf_vars)\n",
    "                operations = []\n",
    "                for i,v in enumerate(tf_vars[0:num_of_vars//2]):\n",
    "                    operations.append(tf_vars[i+num_of_vars//2].assign((v.value()*self.tau) + ((1-self.tau)*tf_vars[i+num_of_vars//2].value())))\n",
    "                self.session.run(operations)\n",
    "            return cost\n",
    "\n",
    "    def add_to_memory(self, add_this):\n",
    "        add_this['state'] = self.player_id * add_this['state']\n",
    "        add_this['next_state'] = self.player_id * add_this['next_state']\n",
    "        self.memory.append(add_this)\n",
    "\n",
    "    def save(self, filename):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(self.session, filename)\n",
    "\n",
    "    def restore(self, filename):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(self.session, filename)\n",
    "\n",
    "    def shutdown(self):\n",
    "        self.session.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work for random player and human first\n",
    "\n",
    "class Player():\n",
    "    \"\"\"\n",
    "    Abstract player class\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.player_id = None\n",
    "        self.observation = None\n",
    "        \n",
    "    def reset(self, player_id):\n",
    "        '''reset to the initial state\n",
    "        '''\n",
    "        self.player_id = player_id\n",
    "        self.observation = None\n",
    "        \n",
    "    def observe(self, observation):\n",
    "        '''\n",
    "        receive raw observation from env and tune it if needed\n",
    "        '''\n",
    "        self.observation = tune_observation_view(observation, self.player_id)\n",
    "        \n",
    "    @staticmethod\n",
    "    def tune_observation_view(observation, player_id):\n",
    "        '''\n",
    "        player_id either 1 or -1. Swap the observation such that 1 means self and -1 means the opponent \n",
    "        e.g. \n",
    "        if player_id = 1, no need to swap the view,\n",
    "        input = array([ 1, -1, 0,\n",
    "                        0,  0, 0,\n",
    "                       -1, -1, 1 ])\n",
    "\n",
    "        output = array([ 1, -1, 0,\n",
    "                         0,  0, 0,\n",
    "                        -1, -1, 1 ])\n",
    "\n",
    "        if player_id = -1, need to swap the view,\n",
    "        input = array([ 1, -1, 0,\n",
    "                        0,  0, 0,\n",
    "                       -1, -1, 1 ])\n",
    "\n",
    "        output = array([-1, 1, 0,\n",
    "                         0, 0, 0,\n",
    "                         1, 1, -1 ])\n",
    "\n",
    "        '''\n",
    "        return observation * player_id\n",
    "\n",
    "    def pick_action(self, observation):\n",
    "        '''different players have different way to pick an action\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def memorize(self, add_this):\n",
    "        '''some players will jot notes, some will not\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def learn(self, board, **kwargs):\n",
    "        '''some players will study, some will not\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    \n",
    "class Human(Player):\n",
    "    '''\n",
    "    choose this player if you want to play the game\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Human, self).__init__()\n",
    "        \n",
    "    def pick_action(self, board, **kwargs):\n",
    "        cell = input('Pick a cell (top left is 0 and bottom right is 8): ')\n",
    "        return cell\n",
    "\n",
    "    \n",
    "class Random_player(Player):\n",
    "    \"\"\"\n",
    "    this player will pick random acion for all situation\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Random_player, self).__init__()\n",
    "        \n",
    "    def pick_action(self, is_action_available):\n",
    "        possible_action_list = np.argwhere(is_action_available == 1).reshape([-1])\n",
    "        return np.random.choice(possible_action_list, 1)[0]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_p1 = Random_player()\n",
    "random_p2 = Random_player()\n",
    "\n",
    "random_p1.pick_action(np.array([1,1,1,1,1,1,1,1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-ab283bd9a1f2>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-ab283bd9a1f2>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    for each record:\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def process_memory(memory, is_terminal_state):\n",
    "    if (len(memory) == memory_size + 1) and (is_terminal_state):\n",
    "        # special handle\n",
    "        pass\n",
    "    \n",
    "    # do all the shifting \n",
    "    for each record:\n",
    "        if player is 2:\n",
    "            swap observation, next_observation, reward\n",
    "        \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-83b213627a81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnext_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer_turn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_terminal_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moriginal_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "memory = np.array([])\n",
    "\n",
    "next_observation, player_turn, reward, is_terminal_state = env.step(action)\n",
    "memory.append([original_observation, action, reward, next_observation, player])\n",
    "\n",
    "if (len(memory) == memory_size + 1) or (is_terminal_state):\n",
    "    # shift reward, state, done, and assign 1 for my round and 2 for the opponent round\n",
    "    memory_for_training = process_memory(memory, is_terminal_state)\n",
    "    if is_terminal_state:\n",
    "        memory = np.array([])\n",
    "    else:\n",
    "        memory = memory[-1]\n",
    "        \n",
    "    agent.learn(memory_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main flow\n",
    "\n",
    "load_weight_path = 'dummy_path'\n",
    "model_weight_path = 'dummy_path'\n",
    "\n",
    "env = make_env()\n",
    "bot1 = Bot(load_weight_path)\n",
    "bot2 = Bot()\n",
    "bot_list = [bot1, bot2]\n",
    "unprocess_memory = np.array([])\n",
    "\n",
    "num_episode = 10\n",
    "\n",
    "\n",
    "for episode in range(num_episode):\n",
    "    env.reset()\n",
    "    \n",
    "    # get initial state\n",
    "    observation, player_turn, reward, is_terminal_state = env.get_current_info()\n",
    "    \n",
    "    while not is_terminal_state:\n",
    "        # if now is player 2's turn, swap the observation\n",
    "        if player_turn == 2:\n",
    "            swapped_observation = swap_observation_view(observation)\n",
    "            action = bot_list[player_turn-1].select_action(swapped_observation)\n",
    "        elif player_turn == 1:\n",
    "            action = bot_list[player_turn-1].select_action(observation)\n",
    "        else:\n",
    "            print('Error in player turn. Current player turn is %s.\\n' % player_turn)\n",
    "        \n",
    "        next_observation, next_player_turn, reward, is_terminal_state = env.step(action)\n",
    "        \n",
    "        unprocess_memory.append([observation, action, reward, next_observation, player_turn])\n",
    "        \n",
    "        if (len(unprocess_memory) == memory_size + 1) or (is_terminal_state):\n",
    "            # shift reward, state, done, and assign 1 for my round and 2 for the opponent round\n",
    "            memory_for_training = process_memory(unprocess_memory, is_terminal_state)\n",
    "            if is_terminal_state:\n",
    "                unprocess_memory = np.array([])\n",
    "            else:\n",
    "                unprocess_memory = unprocess_memory[-1]\n",
    "\n",
    "            bot1.learn(memory_for_training)\n",
    "            \n",
    "        observation, player_turn = next_observation, next_player_turn\n",
    "        \n",
    "    # save the model weight periodically\n",
    "    if (episode + 1) % 50 == 0:\n",
    "        bot1.save_weight(model_weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bot():\n",
    "    def __init__(self, load_weight_path=None):\n",
    "        self.observation['board'] = np.zeros([9, 3])\n",
    "        self.observation['1p_inventory'] = np.array([2, 2, 2])\n",
    "        self.observation['2p_inventory'] = np.array([2, 2, 2])\n",
    "                \n",
    "        self.model = self.build_model()\n",
    "        if load_weight_path is not None:\n",
    "            self.model = self.load_weight()\n",
    "        \n",
    "        self\n",
    "        pass\n",
    "    \n",
    "    def build_model(self):\n",
    "        pass\n",
    "    \n",
    "    def load_model(self):\n",
    "        pass\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot1 = Bot()\n",
    "bot2 = Bot()\n",
    "\n",
    "player_list = [bot1, bot2]\n",
    "save_list = []\n",
    "\n",
    "for episode in range(100):\n",
    "    # reset environment\n",
    "    env.reset()\n",
    "    \n",
    "    # start from the 1st player\n",
    "    player_turn = 1\n",
    "    observation = env.observation\n",
    "    while game not end:\n",
    "        action = player_list[state['player_turn']].pick_action(observation)\n",
    "        \n",
    "        # if the action is not valid, the program will be break      \n",
    "        state, next_observation, reward = env.step(action)\n",
    "        \n",
    "        save_list.append([observation, action, next_observation, reward])\n",
    "        observation = next_observation\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'a' must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c4aaf519bf05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'a' must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "np.random.choice(b, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
