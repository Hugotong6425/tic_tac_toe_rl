{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 1546847731\n"
     ]
    }
   ],
   "source": [
    "seed = 1546847731  # or try a new seed by using: seed = int(time())\n",
    "random.seed(seed)\n",
    "print('Seed: {}'.format(seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Game design\n",
    "The game the Q-agents will need to learn is made of a board with 4 cells. The agent will receive a +1 reward every time it fills a vacant cell, and will receive a -1 penalty when it tries to fill an already filled cell. Game ends when the board is full."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    board = None\n",
    "    board_size = 0\n",
    "    \n",
    "    def __init__(self, board_size=4):\n",
    "        self.board_size = board_size\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.board = np.zeros(self.board_size)\n",
    "    \n",
    "    def play(self, cell):\n",
    "        # returns a tuple: (reward, game_over?)\n",
    "        if self.board[cell] == 0:\n",
    "            self.board[cell] = 1\n",
    "            game_over = len(np.where(self.board == 0)[0]) == 0\n",
    "            return (1,game_over)\n",
    "        else:\n",
    "            return (-1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All possible states:\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 1]\n",
      "[0, 0, 1, 0]\n",
      "[0, 0, 1, 1]\n",
      "[0, 1, 0, 0]\n",
      "[0, 1, 0, 1]\n",
      "[0, 1, 1, 0]\n",
      "[0, 1, 1, 1]\n",
      "[1, 0, 0, 0]\n",
      "[1, 0, 0, 1]\n",
      "[1, 0, 1, 0]\n",
      "[1, 0, 1, 1]\n",
      "[1, 1, 0, 0]\n",
      "[1, 1, 0, 1]\n",
      "[1, 1, 1, 0]\n",
      "[1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def state_to_str(state):\n",
    "    return str(list(map(int,state.tolist())))\n",
    "\n",
    "all_states = list()\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        for k in range(2):\n",
    "            for l in range(2):\n",
    "                s = np.array([i,j,k,l])\n",
    "                all_states.append(state_to_str(s))\n",
    "                \n",
    "print('All possible states:')\n",
    "for s in all_states:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Learning\n",
    "Starting of with a table-based Q-learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_games = 2000\n",
    "epsilon = 0.1\n",
    "gamma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[0, 0, 0, 0]</th>\n",
       "      <th>[0, 0, 0, 1]</th>\n",
       "      <th>[0, 0, 1, 0]</th>\n",
       "      <th>[0, 0, 1, 1]</th>\n",
       "      <th>[0, 1, 0, 0]</th>\n",
       "      <th>[0, 1, 0, 1]</th>\n",
       "      <th>[0, 1, 1, 0]</th>\n",
       "      <th>[0, 1, 1, 1]</th>\n",
       "      <th>[1, 0, 0, 0]</th>\n",
       "      <th>[1, 0, 0, 1]</th>\n",
       "      <th>[1, 0, 1, 0]</th>\n",
       "      <th>[1, 0, 1, 1]</th>\n",
       "      <th>[1, 1, 0, 0]</th>\n",
       "      <th>[1, 1, 0, 1]</th>\n",
       "      <th>[1, 1, 1, 0]</th>\n",
       "      <th>[1, 1, 1, 1]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   [0, 0, 0, 0]  [0, 0, 0, 1]  [0, 0, 1, 0]  [0, 0, 1, 1]  [0, 1, 0, 0]  \\\n",
       "0             0             0             0             0             0   \n",
       "1             0             0             0             0             0   \n",
       "2             0             0             0             0             0   \n",
       "3             0             0             0             0             0   \n",
       "\n",
       "   [0, 1, 0, 1]  [0, 1, 1, 0]  [0, 1, 1, 1]  [1, 0, 0, 0]  [1, 0, 0, 1]  \\\n",
       "0             0             0             0             0             0   \n",
       "1             0             0             0             0             0   \n",
       "2             0             0             0             0             0   \n",
       "3             0             0             0             0             0   \n",
       "\n",
       "   [1, 0, 1, 0]  [1, 0, 1, 1]  [1, 1, 0, 0]  [1, 1, 0, 1]  [1, 1, 1, 0]  \\\n",
       "0             0             0             0             0             0   \n",
       "1             0             0             0             0             0   \n",
       "2             0             0             0             0             0   \n",
       "3             0             0             0             0             0   \n",
       "\n",
       "   [1, 1, 1, 1]  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing the Q-table\n",
    "q_table = pd.DataFrame(0, index=np.arange(4), columns=all_states)\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[0, 0, 0, 0]</th>\n",
       "      <th>[0, 0, 0, 1]</th>\n",
       "      <th>[0, 0, 1, 0]</th>\n",
       "      <th>[0, 0, 1, 1]</th>\n",
       "      <th>[0, 1, 0, 0]</th>\n",
       "      <th>[0, 1, 0, 1]</th>\n",
       "      <th>[0, 1, 1, 0]</th>\n",
       "      <th>[0, 1, 1, 1]</th>\n",
       "      <th>[1, 0, 0, 0]</th>\n",
       "      <th>[1, 0, 0, 1]</th>\n",
       "      <th>[1, 0, 1, 0]</th>\n",
       "      <th>[1, 0, 1, 1]</th>\n",
       "      <th>[1, 1, 0, 0]</th>\n",
       "      <th>[1, 1, 0, 1]</th>\n",
       "      <th>[1, 1, 1, 0]</th>\n",
       "      <th>[1, 1, 1, 1]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   [0, 0, 0, 0]  [0, 0, 0, 1]  [0, 0, 1, 0]  [0, 0, 1, 1]  [0, 1, 0, 0]  \\\n",
       "0             4             3             3             2             3   \n",
       "1             4             1             0             0             0   \n",
       "2             4             3             2             0             3   \n",
       "3             4             2             3             0             0   \n",
       "\n",
       "   [0, 1, 0, 1]  [0, 1, 1, 0]  [0, 1, 1, 1]  [1, 0, 0, 0]  [1, 0, 0, 1]  \\\n",
       "0             2             2             0             2             1   \n",
       "1             0             0             0             3             2   \n",
       "2             0            -1             0             3             2   \n",
       "3             0             0             0             3             0   \n",
       "\n",
       "   [1, 0, 1, 0]  [1, 0, 1, 1]  [1, 1, 0, 0]  [1, 1, 0, 1]  [1, 1, 1, 0]  \\\n",
       "0             1            -1             1             0             0   \n",
       "1             2             1             1             0             0   \n",
       "2             1             0             2             1             0   \n",
       "3             2             0             2             0             1   \n",
       "\n",
       "   [1, 1, 1, 1]  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Letting the agent play and learn:\n",
    "\n",
    "r_list = []  # store the total reward of each game so we can plot it later\n",
    "\n",
    "for g in range(num_of_games):\n",
    "    game_over = False\n",
    "    game.reset()\n",
    "    total_reward = 0\n",
    "    while not game_over:\n",
    "        state = np.copy(game.board)\n",
    "        if random.random() < epsilon:\n",
    "            action = random.randint(0,3)\n",
    "        else:\n",
    "            action = q_table[state_to_str(state)].idxmax()\n",
    "        reward, game_over = game.play(action)\n",
    "        total_reward += reward\n",
    "        if np.sum(game.board) == 4:  # terminal state\n",
    "            next_state_max_q_value = 0\n",
    "        else:\n",
    "            next_state = np.copy(game.board)\n",
    "            next_state_max_q_value = q_table[state_to_str(next_state)].max()\n",
    "        q_table.loc[action,state_to_str(state)] = reward + gamma * next_state_max_q_value\n",
    "    r_list.append(total_reward)\n",
    "    \n",
    "    \n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "board: [0 0 0 0]\tpredicted Q values: [4, 4, 4, 4] \tbest action: 0\tcorrect action? True\n",
      "board: [0 0 0 1]\tpredicted Q values: [3, 1, 3, 2] \tbest action: 0\tcorrect action? True\n",
      "board: [0 0 1 0]\tpredicted Q values: [3, 0, 2, 3] \tbest action: 0\tcorrect action? True\n",
      "board: [0 0 1 1]\tpredicted Q values: [2, 0, 0, 0] \tbest action: 0\tcorrect action? True\n",
      "board: [0 1 0 0]\tpredicted Q values: [3, 0, 3, 0] \tbest action: 0\tcorrect action? True\n",
      "board: [0 1 0 1]\tpredicted Q values: [2, 0, 0, 0] \tbest action: 0\tcorrect action? True\n",
      "board: [0 1 1 0]\tpredicted Q values: [2, 0, -1, 0] \tbest action: 0\tcorrect action? True\n",
      "board: [0 1 1 1]\tpredicted Q values: [0, 0, 0, 0] \tbest action: 0\tcorrect action? True\n",
      "board: [1 0 0 0]\tpredicted Q values: [2, 3, 3, 3] \tbest action: 1\tcorrect action? True\n",
      "board: [1 0 0 1]\tpredicted Q values: [1, 2, 2, 0] \tbest action: 1\tcorrect action? True\n",
      "board: [1 0 1 0]\tpredicted Q values: [1, 2, 1, 2] \tbest action: 1\tcorrect action? True\n",
      "board: [1 0 1 1]\tpredicted Q values: [-1, 1, 0, 0] \tbest action: 1\tcorrect action? True\n",
      "board: [1 1 0 0]\tpredicted Q values: [1, 1, 2, 2] \tbest action: 2\tcorrect action? True\n",
      "board: [1 1 0 1]\tpredicted Q values: [0, 0, 1, 0] \tbest action: 2\tcorrect action? True\n",
      "board: [1 1 1 0]\tpredicted Q values: [0, 0, 0, 1] \tbest action: 3\tcorrect action? True\n"
     ]
    }
   ],
   "source": [
    "# Let's verify that the agent indeed learned a correct startegy by seeing what action it will choose in each one of the possible states:\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        for k in range(2):\n",
    "            for l in range(2):\n",
    "                b = np.array([i,j,k,l])\n",
    "                if len(np.where(b == 0)[0]) != 0:\n",
    "                    action = q_table[state_to_str(b)].idxmax()\n",
    "                    pred = q_table[state_to_str(b)].tolist()\n",
    "                    print('board: {b}\\tpredicted Q values: {p} \\tbest action: {a}\\tcorrect action? {s}'\n",
    "                          .format(b=b,p=pred,a=action,s=b[action]==0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the agent indeed picked up the right way to play the game. Still, when looking at the predicted Q values, we see that there are some states where he didn't pick up the correct Q values. For example, look at the Q table above for the state [0,0,1,0]. The Q value the agent learned for the action \"1\" is lower than that of the action \"2\", and this is obviously wrong, as cell 1 is vacant and cell 2 is not (remember the first cell is cell 0). This is due to the fact that the agent didn't reach this state and selected those actions enough times. This is because the Q learning is a greedy algorithm. and prefers to choose the best action he can rather than explore. We can solve this issue increasing $\\epsilon$ (epsilon), which controls the exploration of this algorithm and was set to 0.1, or by letting the agent play more games.\n",
    "\n",
    "Let's plot the total reward the agent received per game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(range(len(r_list)),r_list)\n",
    "plt.xlabel('Games played')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q Network\n",
    "Moving on to neural-network-based modeling. Let's start by designing the Q network. Remember the output ot the network (self.output) is an array of the predicted Q value for each action taken from the input state (self.states). Comparing to the Q-table algorithm, the output is an entire column of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork:\n",
    "    def __init__(self, hidden_layers_size, gamma, learning_rate, input_size=4, output_size=4):\n",
    "        self.q_target = tf.placeholder(shape=(None,output_size), dtype=tf.float32)\n",
    "        self.r = tf.placeholder(shape=None,dtype=tf.float32)\n",
    "        self.states = tf.placeholder(shape=(None, input_size), dtype=tf.float32)\n",
    "        self.enum_actions = tf.placeholder(shape=(None,2), dtype=tf.int32) \n",
    "        layer = self.states\n",
    "        for l in hidden_layers_size:\n",
    "            layer = tf.layers.dense(inputs=layer, units=l, activation=tf.nn.relu,\n",
    "                                    kernel_initializer=tf.contrib.layers.xavier_initializer(seed=seed))\n",
    "        self.output = tf.layers.dense(inputs=layer, units=output_size,\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer(seed=seed))\n",
    "        self.predictions = tf.gather_nd(self.output,indices=self.enum_actions)\n",
    "        self.labels = self.r + gamma * tf.reduce_max(self.q_target, axis=1)\n",
    "        self.cost = tf.reduce_mean(tf.losses.mean_squared_error(labels=self.labels, predictions=self.predictions))\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(self.cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Designing the Experience Replay memory which will be used, usign a cyclic memory buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    memory = None\n",
    "    counter = 0\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.memory = deque(maxlen=size)\n",
    "\n",
    "    def append(self, element):\n",
    "        self.memory.append(element)\n",
    "        self.counter += 1\n",
    "\n",
    "    def sample(self, n):\n",
    "        return random.sample(self.memory, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Settint up parameters. Notice that here I used gamma = 0.99 and not 1 like in the Q-table algorithm, as the literature recommneds working with a discount factor of $0.9 \\le \\gamma \\le 0.99$. It probably won't matter much in this specific case, but it's good to get used to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_games = 2000\n",
    "epsilon = 0.1\n",
    "gamma = 0.99\n",
    "batch_size = 10\n",
    "memory_size = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the Q-network:\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(seed)\n",
    "qnn = QNetwork(hidden_layers_size=[20,20], gamma=gamma, learning_rate=0.001)\n",
    "memory = ReplayMemory(memory_size)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cost: 1.0163475275039673\n"
     ]
    }
   ],
   "source": [
    "# Training the network. Compare this code to the above Q-table training.\n",
    "r_list = []  \n",
    "c_list = []  # same as r_list, but for the cost\n",
    "\n",
    "counter = 0  # will be used to trigger network training\n",
    "\n",
    "for g in range(num_of_games):\n",
    "    game_over = False\n",
    "    game.reset()\n",
    "    total_reward = 0\n",
    "    while not game_over:\n",
    "        counter += 1\n",
    "        state = np.copy(game.board)\n",
    "        if random.random() < epsilon:\n",
    "            action = random.randint(0,3)\n",
    "        else:\n",
    "            pred = np.squeeze(sess.run(qnn.output,feed_dict={qnn.states: np.expand_dims(game.board,axis=0)}))\n",
    "            action = np.argmax(pred)\n",
    "        reward, game_over = game.play(action)\n",
    "        total_reward += reward\n",
    "        next_state = np.copy(game.board)\n",
    "        memory.append({'state':state,'action':action,'reward':reward,'next_state':next_state,'game_over':game_over})\n",
    "        if counter % batch_size == 0:\n",
    "            # Network training\n",
    "            batch = memory.sample(batch_size)\n",
    "            q_target = sess.run(qnn.output,feed_dict={qnn.states: np.array(list(map(lambda x: x['next_state'], batch)))})\n",
    "            terminals = np.array(list(map(lambda x: x['game_over'], batch)))\n",
    "            for i in range(terminals.size):\n",
    "                if terminals[i]:\n",
    "                    # Remember we use the network's own predictions for the next state while calculatng loss.\n",
    "                    # Terminal states have no Q-value, and so we manually set them to 0, as the network's predictions\n",
    "                    # for these states is meaningless\n",
    "                    q_target[i] = np.zeros(game.board_size)\n",
    "            _, cost = sess.run([qnn.optimizer, qnn.cost], \n",
    "                               feed_dict={qnn.states: np.array(list(map(lambda x: x['state'], batch))),\n",
    "                               qnn.r: np.array(list(map(lambda x: x['reward'], batch))),\n",
    "                               qnn.enum_actions: np.array(list(enumerate(map(lambda x: x['action'], batch)))),\n",
    "                               qnn.q_target: q_target})\n",
    "            c_list.append(cost)\n",
    "    r_list.append(total_reward)\n",
    "print('Final cost: {}'.format(c_list[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
