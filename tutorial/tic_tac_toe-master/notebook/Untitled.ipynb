{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    costs = []  # this will store the costs, so we can plot them later\n",
    "    r1 = []  # same, but for the players total rewards\n",
    "    r2 = []\n",
    "    random.seed(int(time()*1000))\n",
    "    tf.reset_default_graph()\n",
    "    logging.basicConfig(level=logging.WARN, format='%(message)s')\n",
    "\n",
    "    # Initialize players\n",
    "    p1 = players.QPlayer([100,160,160,100],\n",
    "                         learning_batch_size=150, batches_to_q_target_switch=1000,\n",
    "                         gamma=0.95, tau=0.95, memory_size=100000)\n",
    "    p1.restore('./models/q.ckpt')\n",
    "    p1.name = 'Q'\n",
    "\n",
    "\n",
    "    p2 = players.Novice()\n",
    "    p2.name = 'N'\n",
    "    \n",
    "    total_rewards = {p1.name: 0, p2.name: 0}\n",
    "\n",
    "    # Start playing\n",
    "    num_of_games = 400000\n",
    "    for g in range(1,num_of_games+1):\n",
    "        game = Game(p1,p2) if g%2==0 else Game(p2,p1)  # make sure both players play X and O\n",
    "        last_phases = {p1.name: None, p2.name: None}  # will be used to store the last state a player was in\n",
    "        while not game.game_status()['game_over']:\n",
    "            if isinstance(game.active_player(), players.Human):\n",
    "                game.print_board()\n",
    "                print(\"{}'s turn:\".format(game.active_player().name))\n",
    "\n",
    "            # If this is not the first move, store in memory the transition from the last state\n",
    "            # the active player saw to this one\n",
    "            state = np.copy(game.board)\n",
    "            if last_phases[game.active_player().name] is not None:\n",
    "                memory_element = last_phases[game.active_player().name]\n",
    "                memory_element['next_state'] = state\n",
    "                memory_element['game_over'] = False\n",
    "                game.active_player().add_to_memory(memory_element)\n",
    "\n",
    "            # Calculate annealed epsilon\n",
    "            if g <= num_of_games // 4:\n",
    "                max_eps = 0.6\n",
    "            elif g <= num_of_games // 2:\n",
    "                max_eps = 0.01\n",
    "            else:\n",
    "                max_eps = 0.001\n",
    "            min_eps = 0.01 if g <= num_of_games // 2 else 0.0\n",
    "            eps = round(max(max_eps - round(g*(max_eps-min_eps)/num_of_games, 3), min_eps), 3)\n",
    "\n",
    "            # Play and receive reward\n",
    "            action = int(game.active_player().select_cell(state, epsilon=eps))\n",
    "            play_status = game.play(action)\n",
    "            game_over = play_status['game_over']\n",
    "            if play_status['invalid_move']:\n",
    "                r = game.invalid_move_reward\n",
    "            elif game_over:\n",
    "                if play_status['winner'] == 0:\n",
    "                    r = game.tie_reward\n",
    "                else:\n",
    "                    r = game.winning_reward\n",
    "            else:\n",
    "                r = 0\n",
    "\n",
    "            # Store the current state in temporary memory\n",
    "            last_phases[game.active_player().name] = {'state': state,\n",
    "                                                      'action': action,\n",
    "                                                      'reward': r}\n",
    "            total_rewards[game.active_player().name] += r\n",
    "\n",
    "            # Activate learning procedure\n",
    "            cost = game.active_player().learn(learning_rate=0.0001)\n",
    "            if cost is not None:\n",
    "                costs.append(cost)\n",
    "\n",
    "            # Next player's turn, if game hasn't ended\n",
    "            if not game_over:\n",
    "                game.next_player()\n",
    "\n",
    "        # Adding last phase for winning (active) player\n",
    "        memory_element = last_phases[game.active_player().name]\n",
    "        memory_element['next_state'] = np.zeros(9)\n",
    "        memory_element['game_over'] = True\n",
    "        game.active_player().add_to_memory(memory_element)\n",
    "\n",
    "        # Adding last phase for losing (inactive) player\n",
    "        memory_element = last_phases[game.inactive_player().name]\n",
    "        memory_element['next_state'] = np.zeros(9)\n",
    "        memory_element['game_over'] = True\n",
    "        memory_element['reward'] = game.losing_reward\n",
    "        game.inactive_player().add_to_memory(memory_element)\n",
    "\n",
    "        # Print statistics\n",
    "        if g % 100 == 0:\n",
    "            print('Game: {g} | Number of Trainings: {t} | Epsilon: {e} | Average Rewards - {p1}: {r1}, {p2}: {r2}'\n",
    "                  .format(g=g, p1=p1.name, r1=total_rewards[p1.name]/100.0,\n",
    "                          p2=p2.name, r2=total_rewards[p2.name]/100.0,\n",
    "                          t=len(costs), e=eps))\n",
    "            r1.append(total_rewards[p1.name]/100.0)\n",
    "            r2.append(total_rewards[p2.name]/100.0)\n",
    "            total_rewards = {p1.name: 0, p2.name: 0}\n",
    "\n",
    "    # Save trained model and shutdown Tensorflow sessions\n",
    "    p1.save('./models/q.ckpt')\n",
    "    for pp in [p1,p2]:\n",
    "        pp.shutdown()\n",
    "\n",
    "    # Plot graphs\n",
    "    plt.scatter(range(len(costs)),costs)\n",
    "    plt.show()\n",
    "    plt.scatter(range(len(r1)),r1,c='g')\n",
    "    plt.show()\n",
    "    plt.scatter(range(len(r2)), r2, c='r')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play():\n",
    "    random.seed(int(time()))\n",
    "    \n",
    "    # load the RL model\n",
    "    p1 = players.QPlayer([100,160,160,100], learning_batch_size=100, gamma=0.95, tau=0.95,\n",
    "                         batches_to_q_target_switch=100, memory_size=100000)\n",
    "    p1.restore('./models/q.ckpt')\n",
    "    \n",
    "    # player 2 is human\n",
    "    p2 = players.Human()\n",
    "    \n",
    "    for g in range(4):\n",
    "        print('STARTING NEW GAME (#{})\\n-------------'.format(g))\n",
    "        if g%2==0:\n",
    "            game = Game(p1,p2)\n",
    "            print(\"Computer is X (1)\")\n",
    "        else:\n",
    "            game = Game(p2,p1)\n",
    "            print(\"Computer is O (-1)\")\n",
    "        while not game.game_status()['game_over']:\n",
    "            if isinstance(game.active_player(), players.Human):\n",
    "                game.print_board()\n",
    "                print(\"{}'s turn:\".format(game.current_player))\n",
    "            state = np.copy(game.board)\n",
    "            # Force Q-Network to select different starting positions if it plays first\n",
    "            action = int(game.active_player().select_cell(state,epsilon=0.0)) if np.count_nonzero(game.board) > 0 or not isinstance(game.active_player(),players.QPlayer) else random.randint(0,8)\n",
    "            game.play(action)\n",
    "            if not game.game_status()['game_over']:\n",
    "                game.next_player()\n",
    "        print('-------------\\nGAME OVER!')\n",
    "        game.print_board()\n",
    "        print(game.game_status())\n",
    "        print('-------------')\n",
    "\n",
    "#train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Game:\n",
    "    \"\"\"\n",
    "    Tic-Tac-Toe game class\n",
    "    \"\"\"\n",
    "    board = np.zeros(9)\n",
    "    current_player = 1  # first player is 1, second player is -1\n",
    "    player1 = None\n",
    "    player2 = None\n",
    "\n",
    "    _invalid_move_played = False\n",
    "\n",
    "    def __init__(self, player1, player2,\n",
    "                 winning_reward=1,\n",
    "                 losing_reward=-1,\n",
    "                 tie_reward=0,\n",
    "                 invalid_move_reward=-10):\n",
    "        self.player1 = player1\n",
    "        self.player2 = player2\n",
    "        self.player1.player_id = 1\n",
    "        self.player2.player_id = -1\n",
    "        self.winning_reward = winning_reward\n",
    "        self.losing_reward = losing_reward\n",
    "        self.invalid_move_reward = invalid_move_reward\n",
    "        self.tie_reward = tie_reward\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros(9)\n",
    "        self.current_player = 1\n",
    "        self._invalid_move_played = False\n",
    "\n",
    "    def active_player(self):\n",
    "        if self.current_player == 1:\n",
    "            return self.player1\n",
    "        else:\n",
    "            return self.player2\n",
    "\n",
    "    def inactive_player(self):\n",
    "        if self.current_player == -1:\n",
    "            return self.player1\n",
    "        else:\n",
    "            return self.player2\n",
    "\n",
    "    def play(self, cell):\n",
    "        self._invalid_move_played = False\n",
    "        if self.board[cell] != 0:\n",
    "            self._invalid_move_played = True\n",
    "            return {'winner': 0,\n",
    "                    'game_over': False,\n",
    "                    'invalid_move': True}\n",
    "        else:\n",
    "            self.board[cell] = self.current_player\n",
    "        status = self.game_status()\n",
    "        return {'winner': status['winner'],\n",
    "                'game_over': status['game_over'],\n",
    "                'invalid_move': False}\n",
    "\n",
    "    def next_player(self):\n",
    "        if not self._invalid_move_played:\n",
    "            self.current_player *= -1\n",
    "\n",
    "    def game_status(self):\n",
    "        winner = 0\n",
    "        winning_seq = []\n",
    "        winning_options = [[0,1,2],[3,4,5],[6,7,8],\n",
    "                           [0,3,6],[1,4,7],[2,5,8],\n",
    "                           [0,4,8],[2,4,6]]\n",
    "        for seq in winning_options:\n",
    "            s = self.board[seq[0]] + self.board[seq[1]] + self.board[seq[2]]\n",
    "            if abs(s) == 3:\n",
    "                winner = s/3\n",
    "                winning_seq = seq\n",
    "                break\n",
    "        game_over = winner != 0 or len(list(filter(lambda z: z==0, self.board))) == 0\n",
    "        return {'game_over': game_over, 'winner': winner,\n",
    "                'winning_seq': winning_seq, 'board': self.board}\n",
    "\n",
    "    def print_board(self):\n",
    "        row = ' '\n",
    "        status = self.game_status()\n",
    "        for i in reversed(range(9)):\n",
    "            if self.board[i] == 1:\n",
    "                cell = 'x'\n",
    "            elif self.board[i] == -1:\n",
    "                cell = 'o'\n",
    "            else:\n",
    "                cell = ' '\n",
    "            if status['winner'] != 0 and i in status['winning_seq']:\n",
    "                cell = cell.upper()\n",
    "            row += cell + ' '\n",
    "            if i % 3 != 0:\n",
    "                row += '| '\n",
    "            else:\n",
    "                row = row[::-1]\n",
    "                if i != 0:\n",
    "                    row += ' \\n-----------'\n",
    "                print(row)\n",
    "                row = ' '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class QNetwork:\n",
    "    \"\"\"\n",
    "    A Q-Network implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_layers_size, gamma):\n",
    "        self.q_target = tf.placeholder(shape=(None, output_size), dtype=tf.float32)\n",
    "        self.r = tf.placeholder(shape=None, dtype=tf.float32)\n",
    "        self.states = tf.placeholder(shape=(None, input_size), dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=(None, 2), dtype=tf.int32)  # enumerated actions\n",
    "        self.learning_rate = tf.placeholder(shape=[], dtype=tf.float32)\n",
    "        layer = self.states\n",
    "        for l in hidden_layers_size:\n",
    "            layer = tf.layers.dense(inputs=layer, units=l, activation=tf.nn.relu,\n",
    "                                    kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.output = tf.layers.dense(inputs=layer, units=output_size,\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.predictions = tf.gather_nd(self.output, indices=self.actions)\n",
    "        self.labels = self.r + (gamma * tf.reduce_max(self.q_target, axis=1))\n",
    "        self.cost = tf.reduce_mean(tf.losses.mean_squared_error(labels=self.labels, predictions=self.predictions))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    \"\"\"\n",
    "    A cyclic Experience Replay memory buffer\n",
    "    \"\"\"\n",
    "    memory = None\n",
    "    counter = 0\n",
    "\n",
    "    def __init__(self, size, seed=None):\n",
    "        self.memory = deque(maxlen=size)\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def append(self, element):\n",
    "        self.memory.append(element)\n",
    "        self.counter += 1\n",
    "\n",
    "    def sample(self, n, or_less=False):\n",
    "        if or_less and n > self.counter:\n",
    "            n = self.counter\n",
    "        return random.sample(self.memory, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from abc import abstractmethod\n",
    "\n",
    "\n",
    "class Player:\n",
    "    \"\"\"\n",
    "    Base class for all player types\n",
    "    \"\"\"\n",
    "    name = None\n",
    "    player_id = None\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def shutdown(self):\n",
    "        pass\n",
    "\n",
    "    def add_to_memory(self, add_this):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def select_cell(self, board, **kwargs):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def learn(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Human(Player):\n",
    "    \"\"\"\n",
    "    This player type allow a human player to play the game\n",
    "    \"\"\"\n",
    "    def select_cell(self, board, **kwargs):\n",
    "        cell = input(\"Select cell to fill:\\n678\\n345\\n012\\ncell number: \")\n",
    "        return cell\n",
    "\n",
    "    def learn(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Drunk(Player):\n",
    "    \"\"\"\n",
    "    Drunk player always selects a random valid move\n",
    "    \"\"\"\n",
    "    def select_cell(self, board, **kwargs):\n",
    "        available_cells = np.where(board == 0)[0]\n",
    "        return random.choice(available_cells)\n",
    "\n",
    "    def learn(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Novice(Player):\n",
    "    \"\"\"\n",
    "    A more sophisticated bot, which follows the following strategy:\n",
    "    1) If it already has 2-in-a-row, capture the required cell for 3\n",
    "    2) If not, and if the opponent has 2-in-a-row, capture the required cell to prevent hi, from winning\n",
    "    3) Else, select a random vacant cell\n",
    "    \"\"\"\n",
    "    def find_two_of_three(self, board, which_player_id):\n",
    "        cell = None\n",
    "        winning_options = [[0, 1, 2], [3, 4, 5], [6, 7, 8],\n",
    "                           [0, 3, 6], [1, 4, 7], [2, 5, 8],\n",
    "                           [0, 4, 8], [2, 4, 6]]\n",
    "        random.shuffle(winning_options)\n",
    "        for seq in winning_options:\n",
    "            s = board[seq[0]] + board[seq[1]] + board[seq[2]]\n",
    "            if s == 2 * which_player_id:\n",
    "                a = np.array([board[seq[0]], board[seq[1]], board[seq[2]]])\n",
    "                c = np.where(a == 0)[0][0]\n",
    "                cell = seq[c]\n",
    "                break\n",
    "        return cell\n",
    "\n",
    "    def select_cell(self, board, **kwargs):\n",
    "        cell = self.find_two_of_three(board,self.player_id)\n",
    "        if cell is None:\n",
    "            cell = self.find_two_of_three(board,-self.player_id)\n",
    "        if cell is None:\n",
    "            available_cells = np.where(board == 0)[0]\n",
    "            cell = random.choice(available_cells)\n",
    "        return cell\n",
    "\n",
    "    def learn(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class lazy_tensai(Player):\n",
    "    \"\"\"\n",
    "    A reinforcement learning agent, based on Double Deep Q Network model\n",
    "    This class holds two Q-Networks: `qnn` is the learning network, `q_target` is the semi-constant network\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_layers_size, gamma):\n",
    "        \"\"\"\n",
    "        :param hidden_layers_size: an array of integers, specifying the number of layers of the network and their size\n",
    "        :param gamma: the Q-Learning discount factor\n",
    "        :param learning_batch_size: training batch size\n",
    "        :param batches_to_q_target_switch: after how many batches (trainings) should the Q-network be copied to Q-Target\n",
    "        :param tau: a number between 0 and 1, determining how to combine the network and Q-Target when copying is performed\n",
    "        :param memory_size: size of the memory buffer used to keep the training set\n",
    "        \"\"\"\n",
    "\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)  \n",
    "        self.session = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "        self.qnn = dqn.QNetwork(9, 9, hidden_layers_size, gamma)\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        super(lazy_tensai, self).__init__()\n",
    "\n",
    "    def select_cell(self, board, **kwargs):\n",
    "        rnd = random.random()\n",
    "        eps = kwargs['epsilon']\n",
    "        if rnd < eps:\n",
    "            cell = random.randint(0,8)\n",
    "            logging.debug(\"Choosing a random cell: %s [Epsilon = %s]\", cell, eps)\n",
    "        else:\n",
    "            prediction = self.session.run(self.qnn.output,feed_dict={self.qnn.states: np.expand_dims(self.player_id * board, axis=0)})\n",
    "            prediction = np.squeeze(prediction)\n",
    "            cell = np.argmax(prediction)\n",
    "            logging.debug(\"Predicting next cell - board: %s | player ID: %s | prediction: %s | cell: %s [Epsilon = %s]\", board, prediction, cell, eps)\n",
    "        return cell\n",
    "\n",
    "    def restore(self, filename):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(self.session, filename)\n",
    "        \n",
    "    def learn(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class QPlayer(Player):\n",
    "    \"\"\"\n",
    "    A reinforcement learning agent, based on Double Deep Q Network model\n",
    "    This class holds two Q-Networks: `qnn` is the learning network, `q_target` is the semi-constant network\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_layers_size, gamma, learning_batch_size,\n",
    "                 batches_to_q_target_switch, tau, memory_size):\n",
    "        \"\"\"\n",
    "        :param hidden_layers_size: an array of integers, specifying the number of layers of the network and their size\n",
    "        :param gamma: the Q-Learning discount factor\n",
    "        :param learning_batch_size: training batch size\n",
    "        :param batches_to_q_target_switch: after how many batches (trainings) should the Q-network be copied to Q-Target\n",
    "        :param tau: a number between 0 and 1, determining how to combine the network and Q-Target when copying is performed\n",
    "        :param memory_size: size of the memory buffer used to keep the training set\n",
    "        \"\"\"\n",
    "        self.learning_batch_size = learning_batch_size\n",
    "        self.batches_to_q_target_switch = batches_to_q_target_switch\n",
    "        self.tau = tau\n",
    "        self.learn_counter = 0\n",
    "        self.counter = 0\n",
    "\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)  \n",
    "        self.session = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "        self.memory = dqn.ReplayMemory(memory_size)\n",
    "        self.qnn = dqn.QNetwork(9, 9, hidden_layers_size, gamma)\n",
    "        self.q_target = dqn.QNetwork(9, 9, hidden_layers_size, gamma)\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        super(QPlayer, self).__init__()\n",
    "\n",
    "    def select_cell(self, board, **kwargs):\n",
    "        rnd = random.random()\n",
    "        eps = kwargs['epsilon']\n",
    "        self.counter += 1\n",
    "        if rnd < eps:\n",
    "            cell = random.randint(0,8)\n",
    "            logging.debug(\"Choosing a random cell: %s [Epsilon = %s]\", cell, eps)\n",
    "        else:\n",
    "            prediction = self.session.run(self.qnn.output,feed_dict={self.qnn.states: np.expand_dims(self.player_id * board, axis=0)})\n",
    "            prediction = np.squeeze(prediction)\n",
    "            cell = np.argmax(prediction)\n",
    "            logging.debug(\"Predicting next cell - board: %s | player ID: %s | prediction: %s | cell: %s [Epsilon = %s]\", board, prediction, cell, eps)\n",
    "        return cell\n",
    "\n",
    "    @staticmethod\n",
    "    def _fetch_from_batch(batch, key, enum=False):\n",
    "        if enum:\n",
    "            return np.array(list(enumerate(map(lambda x: x[key], batch))))\n",
    "        else:\n",
    "            return np.array(list(map(lambda x: x[key], batch)))\n",
    "\n",
    "    def learn(self, **kwargs):\n",
    "        logging.debug('Memory counter = %s', self.memory.counter)\n",
    "        self.learn_counter += 1\n",
    "        if self.learn_counter % self.learning_batch_size != 0 or self.memory.counter < self.learning_batch_size:\n",
    "            pass\n",
    "        else:\n",
    "            logging.debug('Starting learning procedure')\n",
    "            batch = self.memory.sample(self.learning_batch_size)\n",
    "            qt = self.session.run(self.q_target.output,feed_dict={self.q_target.states: self._fetch_from_batch(batch,'next_state')})\n",
    "            terminals = self._fetch_from_batch(batch,'game_over')\n",
    "            for i in range(terminals.size):\n",
    "                if terminals[i]:\n",
    "                    qt[i] = np.zeros(9)  # manually setting q-target values of terminal states to 0\n",
    "            lr = kwargs['learning_rate']\n",
    "            _, cost = self.session.run([self.qnn.optimizer, self.qnn.cost],\n",
    "                                       feed_dict={self.qnn.states: self._fetch_from_batch(batch,'state'),\n",
    "                                                  self.qnn.r: self._fetch_from_batch(batch,'reward'),\n",
    "                                                  self.qnn.actions: self._fetch_from_batch(batch, 'action', enum=True),\n",
    "                                                  self.qnn.q_target: qt,\n",
    "                                                  self.qnn.learning_rate: lr})\n",
    "            logging.info('Batch number: %s | Q-Network cost: %s | Learning rate: %s',\n",
    "                         self.learn_counter % self.learning_batch_size, cost, lr)\n",
    "            if self.memory.counter % (self.batches_to_q_target_switch * self.learning_batch_size) == 0:\n",
    "                logging.info('Copying Q-Network to Q-Target')\n",
    "                tf_vars = tf.trainable_variables()\n",
    "                num_of_vars = len(tf_vars)\n",
    "                operations = []\n",
    "                for i,v in enumerate(tf_vars[0:num_of_vars//2]):\n",
    "                    operations.append(tf_vars[i+num_of_vars//2].assign((v.value()*self.tau) + ((1-self.tau)*tf_vars[i+num_of_vars//2].value())))\n",
    "                self.session.run(operations)\n",
    "            return cost\n",
    "\n",
    "    def add_to_memory(self, add_this):\n",
    "        add_this['state'] = self.player_id * add_this['state']\n",
    "        add_this['next_state'] = self.player_id * add_this['next_state']\n",
    "        self.memory.append(add_this)\n",
    "\n",
    "    def save(self, filename):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(self.session, filename)\n",
    "\n",
    "    def restore(self, filename):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(self.session, filename)\n",
    "\n",
    "    def shutdown(self):\n",
    "        self.session.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'players' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-08320e732499>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Initialize players\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     p1 = players.QPlayer([100,160,160,100],\n\u001b[0m\u001b[1;32m     11\u001b[0m                          \u001b[0mlearning_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches_to_q_target_switch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                          gamma=0.95, tau=0.95, memory_size=100000)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'players' is not defined"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
